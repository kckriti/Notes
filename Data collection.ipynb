{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb207bf-9954-40ed-9c3b-a3dd07192d96",
   "metadata": {},
   "source": [
    "## The Ultimate Guide to Data Collection \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4744972f-da26-4c7b-9378-265cd3e9515b",
   "metadata": {},
   "source": [
    "### Why is Data Collection Important?\n",
    "Data collection is the foundation of data analysis. Accurate and clean data helps:\n",
    "- Understand trends and patterns.\n",
    "- Support decision-making processes.\n",
    "- Build predictive models and algorithms.\n",
    "\n",
    "As a data analyst, knowing how to collect data efficiently and ethically is key to success.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a429ccf-de5b-456d-b003-e09b4d1b1561",
   "metadata": {},
   "source": [
    "### Step 1: Understand Your Data Requirements\n",
    "Before diving into code, ask yourself:\n",
    "What data do you need? (Structured, semi-structured, or unstructured?)\n",
    "Where can you find it? (Files, APIs, databases, or websites?)\n",
    "What format is the data in? (CSV, JSON, XML, etc.)\n",
    "What are the constraints? (Size, access permissions, frequency of collection?)\n",
    "Take the time to answer these questions as they guide your data collection strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582dd8b9-5190-4ce3-ac70-ec36fa33910d",
   "metadata": {},
   "source": [
    "### Step 2: Set Up Your Python Environment\n",
    "Before you begin collecting data, ensure you have the necessary Python libraries installed. Use the following command to install them:\n",
    "!pip install pandas requests beautifulsoup4 selenium sqlalchemy\n",
    "\n",
    "You might also need additional libraries like:\n",
    "lxml: For parsing XML.\n",
    "openpyxl: For working with Excel files.\n",
    "pymysql or psycopg2: For database connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677568db-05d1-429b-9634-84990ae0d748",
   "metadata": {},
   "source": [
    "### Step 3: Collecting Data from Different Sources\n",
    "1. Collecting Data from Local Files\n",
    "Python’s pandas library makes it easy to read data from files like CSV, Excel, or JSON.\n",
    "\n",
    "Example: Reading a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903ec24-305c-42b1-a264-b0f87979baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a CSV file\n",
    "data = pd.read_csv(\"file.csv\")\n",
    "print(data.head())\n",
    "\n",
    "Example: Reading an Excel File\n",
    "data = pd.read_excel(\"file.xlsx\")\n",
    "print(data.head())\n",
    "\n",
    "Example: Reading a JSON File\n",
    "with open(\"file.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(data)\n",
    "\n",
    "# Note: Always check file encoding and handle errors gracefully using the try-except block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33839c73-9330-414c-893d-304c5c4347d4",
   "metadata": {},
   "source": [
    "2. Collecting Data from APIs\n",
    "APIs allow you to access data programmatically. Python’s requests library makes this easy.\n",
    "\n",
    "Example: Accessing a Public API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb70f3-3131-4e17-9dcf-6077db16d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.example.com/data\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9cf7c-ff23-4afc-ad87-694522cc3d77",
   "metadata": {},
   "source": [
    "### Adding Authentication\n",
    "Some APIs require authentication (e.g., API keys or OAuth tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6774c-a91e-40f7-8422-43460cbcb152",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": \"Bearer YOUR_API_KEY\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Tip: Always respect API rate limits and read the API documentation carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985c91c-fc7d-438e-a9c8-4faf0438d5d8",
   "metadata": {},
   "source": [
    "3. Web Scraping\n",
    "When data isn’t available via APIs, web scraping can help. Use libraries like BeautifulSoup for static websites and Selenium for dynamic ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e15f63-5308-4eef-a3e3-9cddb7367751",
   "metadata": {},
   "source": [
    "### Step 4: Data Cleaning and Validation\n",
    "Once you have collected the data, it’s essential to clean and validate it.\n",
    "\n",
    "Tip: Use exploratory data analysis (EDA) tools like pandas-profiling for quick insights into your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab7bc8-0836-4a1e-95c1-452a45bd697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Checking for Missing Values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Example: Filling Missing Values\n",
    "data = data.fillna(\"default_value\")\n",
    "\n",
    "# Example: Removing Duplicates\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86d4ff-8bee-4411-9558-b112113d0b11",
   "metadata": {},
   "source": [
    "### Step 5: Save the Data\n",
    "Store the data for future analysis. Use formats like CSV, Excel, or JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaaec5c-b1cc-4b45-a75f-f7c5f731a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Saving as CSV\n",
    "data.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249a8fa-687d-462d-b41b-d17c88b3563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Saving as JSON\n",
    "data.to_json(\"output.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c76dc7-60a7-4f1c-84fe-6eafe6142196",
   "metadata": {},
   "source": [
    "### Step 6: Automate the Process\n",
    "Use Python schedulers or task managers to automate recurring data collection.\n",
    "\n",
    "Example: Automating with APScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e66937-d077-4d6d-899c-96287d1a6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "\n",
    "def collect_data():\n",
    "    print(\"Data collected at regular intervals!\")\n",
    "\n",
    "scheduler = BlockingScheduler()\n",
    "scheduler.add_job(collect_data, 'interval', hours=1)\n",
    "scheduler.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae377c55-655b-4ffe-a425-d5514edd4f85",
   "metadata": {},
   "source": [
    "### Step 7: Best Practices\n",
    "Document Everything: \n",
    "- Record the source, date, and method of collection.\n",
    "- Handle Errors Gracefully: Use try-except blocks to catch errors.\n",
    "- Be Ethical: Always comply with web scraping and API usage policies.\n",
    "- Optimize Performance: Use efficient libraries like numpy for large datasets.\n",
    "- Secure Credentials: Store sensitive information (e.g., API keys) in environment variables.\n",
    "\n",
    "Example Project: Scraping IMDb Top Movies\n",
    "Here’s a complete example project to collect data from IMDb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "170669d7-396a-49dc-95c6-404de3a3ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.imdb.com/chart/top/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "movies = []\n",
    "for row in soup.find_all(\"tr\"):\n",
    "    title = row.find(\"td\", class_=\"titleColumn\")\n",
    "    rating = row.find(\"td\", class_=\"ratingColumn imdbRating\")\n",
    "    if title and rating:\n",
    "        movies.append({\n",
    "            \"title\": title.a.text.strip(),\n",
    "            \"rating\": float(rating.strong.text.strip())\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(movies)\n",
    "df.to_csv(\"imdb_top_movies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8ffbc-af22-4765-9f17-25931279a14d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Data collection is a critical skill for any data analyst. By following these steps and using the tools and techniques described, you can confidently collect, clean, and store data for your projects. With practice, you'll develop efficient workflows that save time and improve the quality of your analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
